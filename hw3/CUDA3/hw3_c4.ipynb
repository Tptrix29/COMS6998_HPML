{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QMm1yn0ty_nO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /opt/conda/lib/python3.10/site-packages (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing needed package (triton)\n",
    "! pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sfrZldkreUN9"
   },
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UiqlzD27zWbb"
   },
   "outputs": [],
   "source": [
    "# Golbal Variables\n",
    "DEVICE = \"cuda\"\n",
    "C_OUT = 64\n",
    "C_IN = 3\n",
    "H = 1024\n",
    "W = 1024\n",
    "FH = 3\n",
    "FW = 3\n",
    "P = 1\n",
    "BLOCK_H, BLOCK_W = 16, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Tv7-OpfzcKd"
   },
   "outputs": [],
   "source": [
    "# Making torch tensors\n",
    "tensor_I = torch.rand(1, C_IN, H, W, device=DEVICE) # Input, assuming that batch_size is one\n",
    "tensor_F = torch.rand(C_OUT, C_IN, FH, FW , device=DEVICE) # Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0LQaM2aP1zYK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# This is the result from Convolutional Layer provided by Torch\n",
    "# Use this for correctness check\n",
    "golden_out = F.conv2d(tensor_I, tensor_F, padding=1)\n",
    "print(golden_out.shape) # (1, C_OUT, OUT_H, OUT_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def my_triton_kernel(\n",
    "    input_ptr, kernel_ptr, output_ptr,\n",
    "    H, W, C_IN, C_OUT, FH, FW, OUT_H, OUT_W,\n",
    "    stride_h, stride_w, padding_h, padding_w,\n",
    "    BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n",
    "):\n",
    "    # TODO: Complete the triton kernel that does convolution\n",
    "    # Pointers to the output tile\n",
    "    oh = tl.program_id(0)\n",
    "    ow = tl.program_id(1)\n",
    "    co = tl.program_id(2)\n",
    "\n",
    "    h_offsets = oh * BLOCK_H + tl.arange(0, BLOCK_H)\n",
    "    w_offsets = ow * BLOCK_W + tl.arange(0, BLOCK_W)\n",
    "\n",
    "    # Initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_H, BLOCK_W), dtype=tl.float32)\n",
    "\n",
    "    for ci in range(0, C_IN):\n",
    "        for fh in range(FH):\n",
    "            for fw in range(FW):\n",
    "                h_in = h_offsets + fh - padding_h\n",
    "                w_in = w_offsets + fw - padding_w\n",
    "\n",
    "                mask_h = (h_in >= 0) & (h_in < H)\n",
    "                mask_w = (w_in >= 0) & (w_in < W)\n",
    "                mask = mask_h[:, None] & mask_w[None, :]\n",
    "\n",
    "                input_idx = (\n",
    "                    ci * H * W\n",
    "                    + h_in[:, None] * W\n",
    "                    + w_in[None, :]\n",
    "                )\n",
    "                input_val = tl.load(input_ptr + input_idx, mask=mask, other=0.0)\n",
    "\n",
    "                kernel_idx = (\n",
    "                    co * C_IN * FH * FW\n",
    "                    + ci * FH * FW\n",
    "                    + fh * FW\n",
    "                    + fw\n",
    "                )\n",
    "                kernel_val = tl.load(kernel_ptr + kernel_idx)\n",
    "\n",
    "                acc += input_val * kernel_val\n",
    "\n",
    "    # Write back\n",
    "    output_idx = (\n",
    "        co * OUT_H * OUT_W\n",
    "        + h_offsets[:, None] * OUT_W\n",
    "        + w_offsets[None, :]\n",
    "    )\n",
    "    mask_h_out = h_offsets < OUT_H\n",
    "    mask_w_out = w_offsets < OUT_W\n",
    "    mask_out = mask_h_out[:, None] & mask_w_out[None, :]\n",
    "\n",
    "    tl.store(output_ptr + output_idx, acc, mask=mask_out)\n",
    "\n",
    "def my_conv2d(input, kernel):\n",
    "    # TODO: Initializing some variables\n",
    "    B, C_IN, H, W = input.shape\n",
    "    C_OUT, _, FH, FW = kernel.shape\n",
    "    OUT_H = H\n",
    "    OUT_W = W\n",
    "\n",
    "    # TODO: Calculate output dimension & Allocate output tensor\n",
    "    output = torch.empty((B, C_OUT, OUT_H, OUT_W), device=input.device, dtype=input.dtype)\n",
    "\n",
    "    # Flatten input and kernel for Triton\n",
    "    input_flat = input.view(-1)\n",
    "    kernel_flat = kernel.view(-1)\n",
    "    output_flat = output.view(-1)\n",
    "\n",
    "    # TODO: Define grid\n",
    "    grid = (\n",
    "        triton.cdiv(OUT_H, BLOCK_H),\n",
    "        triton.cdiv(OUT_W, BLOCK_W),\n",
    "        C_OUT\n",
    "    )\n",
    "    # TODO: Call the triton kernel (my_triton_kernel) and measure execution time\n",
    "    # Warm up\n",
    "    my_triton_kernel[grid](\n",
    "        input_flat, kernel_flat, output_flat,\n",
    "        H, W, C_IN, C_OUT, FH, FW, OUT_H, OUT_W,\n",
    "        1, 1, 1, 1,\n",
    "        BLOCK_H=BLOCK_H, BLOCK_W=BLOCK_W\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Measure execution time\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start_event.record()\n",
    "    my_triton_kernel[grid](\n",
    "        input_flat, kernel_flat, output_flat,\n",
    "        H, W, C_IN, C_OUT, FH, FW, OUT_H, OUT_W,\n",
    "        1, 1, 1, 1,\n",
    "        BLOCK_H=BLOCK_H, BLOCK_W=BLOCK_W\n",
    "    )\n",
    "    # Wait for the events to be recorded!\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    end_event.record()\n",
    "\n",
    "    # Measure execution time in milliseconds\n",
    "    execution_time = start_event.elapsed_time(end_event)\n",
    "\n",
    "    # TODO: Return output (output should include execution time)\n",
    "    return output, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JKKBtW7Uz7g3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for triton kernel:  4.057 ms\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Comparing the result from my_conv2d and Conv from torch\n",
    "my_output, execution_time = my_conv2d(tensor_I, tensor_F)\n",
    "torch.testing.assert_close(golden_out, my_output) # Assert statement should be passed\n",
    "# Printing the execution time\n",
    "print(f\"Execution Time for triton kernel: {execution_time: .3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOwWH6PJ8v6nh8WTTD4sAIT",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
